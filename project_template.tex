\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\usepackage{amsmath,amssymb}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{extarrows}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{setspace}

\renewcommand{\baselinestretch}{1.5} 
\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand\diag{\text{diag}}
\bibliographystyle{plainnat}

\usepackage{color}
\usepackage{ulem}
\newcommand{\vmdel}[1]{\sout{#1}}
\newcommand{\vmadd}[1]{\textbf{\color{red}{#1}}}
\newcommand{\vmcomment}[1]{({\color{blue}{VM's comment:}} \textbf{\color{blue}{#1}})}


\begin{document}
%%\maketitle

\begin{center}
  {\LARGE Controlling False Discovery Rate via Knockoffs}\\\ \\
  {Peiran Liu \\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}



\begin{abstract}
  Put your project summary here.
\end{abstract}

\section{Introduction}
Hypothesis Testing is of great interest \vmdel{for} \vmadd{to} statisticians, and testing simultaneously a family of hypotheses is essential in many areas, including variable selection, multiple comparisons\vmadd{,} etc. However, unguarded use of single testing\vmdel{s} procedure \vmcomment{What is a single testing procedure?} will result in a greatly amplified type I error. 
In practice, rarely we are interested only in whether all hypotheses are jointly true or not, which is testing combination of all null hypotheses \vmcomment{I didn't understand the part after ``whicn"}. 
For most applications, we \vmdel{do} \vmadd{perform} inference\vmdel{s} about single hypotheses, and wish to decide which ones can be rejected\vmdel{,} \vmadd{and} which can be called discoveries. 
\vmcomment{Add references and maybe a concrete scientific example.}

Traditionally, in such multiple hypotheses testing problems, we are interested in controlling the probability of erroneously rejecting the null hypotheses. 
In the most conservative \vmdel{prospect} \vmadd{scenario}, scientists \vmdel{we}\vmadd{a}re interested in controlling the probability of erroneously rejecting even one of the true null hypotheses, which is defined as familywise error-rate (FWE). 
This criterion was widely used\vmadd{, with methodology development contributions from} \vmdel{including} \cite{hochberg2009multiple}\vmdel{,} \vmadd{and} \cite{westfall1993resampling} \vmadd{to name a few}. Controlling FWE requires each individual tests to be conducted at lower levels, such as Bonferroni procedures \vmcomment{Not clear what do you mean by levels; also Bonferroni proceduere is not defined and lacks a reference}. 

However, with controlling FWE, the power of detecting true features will be greatly reduced as the number of hypotheses in the family increases, even when more powerful FWE controlling procedures are applied. 
\vmdel{In this point of view} \vmadd{To overcome this shortcoming}, instead of considering controlling FWE as the criterion, scientists are more interested in controlling the proportion of erroneous rejections among all rejections, which \vmdel{incurs} \vmadd{brings us to} the 
\vmdel{idea} \vmadd{notion} of False Discovery Rate.

\subsection{False Discovery Rate}
False Discovery Rate, suggested by \cite{benjamini1995controlling}, is a different point of view \vmdel{for} \vmadd{of} how to look at those erroneous rejections. 
Instead of controlling the probability of erroneously rejecting even one hypothesis, Benjamini and Hochberg control \vmdel{the erroneous rejections by controlling} the proportion of erroneous rejections among all rejections, which is the definition of False Discovery Rate. 

In the case where all hypotheses are true, controlling the FDR is equivalent to controlling the FWE, since when all hypotheses are true, any rejection will be erroneous rejection, which means that False Discovery Rate is 1 when there is any rejection and 0 when there is not. 
However, when the number of true null hypotheses is less than all hypotheses, controlling the FDR will not also control\vmdel{s} FWE. 

To explain the definition of FDR clearer, assume we have $m$ different hypotheses to test, \vmdel{and} of which $m_0$ are true. For a\vmdel{ny} \vmadd{multiple testing} procedure \vmdel{in doing this multiple tests}, \vmadd{suppose we record} \vmdel{if} the result \vmdel{is} in \textbf{Table }\ref{Tbl:1}:
\begin{table}[!htb]\label{Tbl:1}
	\centering
	\caption{Summary of Errors in Multiple Testing}
	\label{my-label}
	\begin{tabular}{l|ccc}
		& \specialcell{Declared\\ non-significant} & \specialcell{Declared\\ significant} & Total \\ \hline
		True Null Hypothes\vmdel{i}\vmadd{e}s	& U & V & $m_0$ \\
		Non-True null Hypothes\vmdel{i}\vmadd{e}s	& T & S & $m-m_0$ \\
		& $m-R$ & R & m
	\end{tabular}
\end{table}

Then, the definition of False Discover Rate is the proportion of rejected true nulls among all rejected hypothesis, which is a random variable $Q = V/(V+S)$. Naturally, if no hypothesis is rejected, then we can define $Q = 0$. 
\vmcomment{Only vectors and matrices should be bolded.}

\subsection{Controlling False Discovery Rate in Variable Selection}
Similar to the ideas in multiple testing problems discussed in previous sections, \cite{barber2015controlling} define\vmdel{s} the false discovery rate in variable selection problems\vmdel{, which is} \vmadd{---} selecting features in regression models. Specifically, consider the linear regression model\vmadd{:}
\begin{align}\label{eq:1}
& \bm{y} = \bm{X}\bm{\beta}+\bm{z}\vmadd{,}
\end{align}
where as usual, \vmdel{variables} $\bm{y}\in\mathbb{R}^n$ is a vector of responses, $\bm{X}\in \mathbb{R}^{n\times p}$ is a known design matrix, \vmdel{and} $\bm{\beta}\in \mathbb{R}^p$ is the unknown vector of coefficients\vmadd{,} and $\bm{z}\sim \mathcal{N}(\bm{0}, \sigma^2\bm{I})$ is Gaussian noise. In this paper, \vmadd{the} authors restrict their attention to the case where $n\geq p$ as otherwise the model would not even be identifiable.

In practice, it's often the case that the size of true support of parameters $\bm{\beta}$ is small, that is, only a few features in $\bm{X}$ are expected to be associated with the outcome $\mathbf{y}$ of interest. In terms of the linear model, this means that $|\{j; \beta_j \neq 0 \}|$ is small. 
The variable selection problem \vmdel{is that} \vmadd{amounts to} developing a procedure to choose $S\subset\{1,\dots, p\}$, such that \vmdel{more} \vmadd{most} true features, whose $\beta_j$ is non-zero, are selected, and mo\vmdel{re}\vmadd{st} null features are not selected in set $S$. In practice, we want to control the proportion of incorrectly selected features into our model. Similarly, we define the false discovery rate in variable selection as the proportion of erroneously selected features among all selected features.

If we consider a multiple testing problem with $p$ hypotheses\vmadd{:} $H_j: \bm{\beta}_j = 0$, where $j=1,2,\dots,p$, \vmdel{and} \vmadd{then} rejecting hypotheses $H_j$ is statistically equivalent to selecting variable (feature) j. Th\vmdel{en}\vmadd{is makes} the false discovery rate defined in variable selection problem \vmdel{is} equivalent to that in multiple testing problem. 

\subsection{The Knockoff Filter}
For controlling False Discovery Rate in \vmadd{the} variable selection problem, \cite{barber2015controlling} proposed a general FDR controlling procedure via knockoff variables. To summarize the ideas in this paper, they construct knockoff variables, which \vmdel{constructed} \vmadd{formed} to imitate the correlation structure of the original features. 
On the other hand, the original features should \vmdel{be} not \vmadd{be} so similar to its knockoff features, which will make its knockoff works better as a standard in the next variable selection procedure, such as Lasso \cite{tibshirani1996regression} applied in this paper.
\vmcomment{There is lot going on in the last sentence; break it into two or more sentences and explain clearly.}

\vmadd{The} \vmdel{M}\vmadd{m}ethod proposed \vmdel{in} \vmadd{by} \cite{barber2015controlling} is guaranteed to work under any fixed design $\bm{X} \in \mathbb{R}^{n\times p}$ as long as $n>p$ and $\bm{y}$ is \vmadd{a} linear \vmadd{function of covariates with added} Gaussian \vmdel{response} \vmadd{noise}. 
When these assumptions are satisfied, the procedure suggested \vmdel{in} \vmadd{by} \cite{barber2015controlling} will theoretically control the false discovery rate with certain levels\vmdel{,} \vmadd{.} \vmdel{and what's more} \vmadd{Moreover}, the power \vmadd{of the knockoff filter} \vmdel{over}\vmadd{out}-performs \vmdel{that in} \vmadd{the power of} \cite{benjamini1995controlling}\vmadd{'s}\vmdel{,} and \vmdel{in} \cite{benjamini2001control}\vmadd{'s,} in simulation studies.

\section{Methods}
\subsection{Construct\vmadd{ing} Knockoffs}
For each feature $\bm{X}_j$ in the model, which is the $j$th column of $\bm{X}$, we construct a "knockoff" feature $\bm{\tilde X}_j$, such that 
\begin{align*}
& \tilde {\bm{X}}^T\tilde {\bm{X}} = \bm{X}^T\bm{X} = \bm{\Sigma}\vmadd{,} \\
& \tilde {\bm{X}}^T\bm{X} = \bm{\Sigma} - \text{diag}{(\bm{s})}\vmadd{,}
\end{align*}
where $\bm{s}$ is a $p$-dimensional non-negative vector. 
By this construction, we \vmdel{can} have three properties of this knockoff features. 
The first is that the knockoff features $\bm{\tilde{X}}$ have the same correlation structure of the original features. 
Also, the correlation between distinct original and knockoff features is the same as the correlation between the corresponding distinct features\vmadd{,} because $\bm{X}_j^T\tilde{\bm{X}}_k = \bm{X}_j^T\bm{X}_k$ for all $j\neq k$. 
However, if we compare a feature $\bm{X}_j$ and its knockoff $\bm{\tilde X}_j$, the correlation is then:
\begin{align*}
& \bm{X}_j^T\tilde{\bm{X}}_j = \bm{\Sigma}_{jj} - s_j = 1-s_j\vmadd{,}
\end{align*}
when $\bm{X}$ is normalized.\vmcomment{Define ``normalized".} 
If we choose relatively large $s_j$, the knockoff features will \vmdel{be} not \vmadd{be} so similar to its original features. 
Then, when we compare test statistic\vmadd{s?} of one original true feature and its knockoff variable, the test statistic of the knockoff variable will be similar to the statistics of the null features, which are expected to be different from true features. Thus, with this construction, the knockoff features can be a good reference in the variable selection process.
\vmcomment{What does it mean to be ``a good reference"?} 

To choose the largest $s$ subject to the constraints $\diag\{\bm{s} \}\preceq 2\Sigma$ and $0\leq s_j\leq 1$, two different criterion can be applied. 
\begin{itemize}
\item {\it Equi-Correlated knockoffs:} If we assume the correlation between features and their knockoffs are equal, which means that all $s_j$ are equal, then we should choose $s_j$ in 
\vmdel{(\ref{eq:2.1})} \vmadd{as}
\begin{align}\label{eq:2.1}
& s_j = 2\lambda_{\min}(\Sigma)\wedge 1 \vmadd{.}
\end{align}
\item {\it SDP knockoffs:} Instead of assuming the correlation\vmadd{s} are equal, selecting $s$ so that the average correlation between features and their knockoffs is minimized is the second choice. This is done by solving the \vmadd{following} convex problem \vmdel{(\ref{eq:2.2}):}
\begin{align}\label{eq:2.2}
&\begin{aligned}
& \text{Minimize }\sum_j(1-s_j)\\
&\text{Subject to }\quad 0\leq s_j\leq 1, \textbf{diag}(s) \preceq 2\Sigma\vmadd{.}
\end{aligned}
\end{align}
\end{itemize}

The construction of $\bm{\tilde X}$ can be done automatically by choos\vmdel{e}\vmadd{ing} $\bm{s}\in\mathbb{R}_+^p$ satisfying $\diag\{\bm{s} \}\preceq 2\Sigma$, which is a feasible convex optimization problem \citep{boyd2004convex}. $\bm\tilde{X}$ can be constructed as follows:
\begin{align}\label{eq:4}
& \tilde{\bm{X}} = \bm{X}(\bm{I}- \bm{\Sigma}^{-1}\text{diag}\{\bm{s} \}) + \bm{\tilde U}C\vmadd{,} 
\end{align}
where $\tilde{\bm{U}}$ is orthonormal $n\times p$ matrix to $\bm{X}$, and $\bm{C}$ is a Cholesky decomposition of $2\diag\{\bm{s}\} - \diag\{\bm{s}\} \Sigma^{-1} \diag\{\bm{s}\}$.
\vmcomment{Not clear where/how you get $\tilde{\bm{U}}$.}

\subsection{Calculat\vmadd{ing} Statistics for Each Pair of Variables via Lasso}
Lasso (\cite{tibshirani1996regression}) is a well-known variable selection methods in linear regression\vmdel{s}, which is known to be asymptotically accurate for both variable selection and coefficient estimation. From the variable selection perspective, we consider the Lasso 
\vmdel{model (\ref{model:6})} \vmadd{estimator}:
\begin{align}\label{model:6}
& \hat{\bm{\beta}}(\lambda) = \arg\min_{\bm b} \left\{\frac{1}{2}\|\bm{y} - \bm{X}\bm{b} \|_2^2 + \lambda\|\bm{b}\|_1 \right\}\vmadd{.}
\end{align}
\vmdel{Correspondingly,} \vmadd{Let's define} \vmdel{the} \vmadd{a} test statistics for feature j \vmdel{is then} \vmadd{as follows}:
\begin{align}
& Z_j = \sup\{\lambda; \hat{\bm{\beta}}_j(\lambda)=0 \}\vmadd{,}
\end{align}
which is likely to be small for null features but large for signals.
However, to quantify this and \vmadd{to} choose an appropriate threshold for variable selection, we need to use the knockoff variables to calibrate our threshold. Thus, if we replace $\bm{X}$ by $[\bm{X} \bm{\tilde{X}}]$ in (\ref{model:6}), we can get paired statistics $(Z_j, \tilde{Z}_j)$ as references. Thus, we can use the following statistics:
\begin{align}\label{eq:7}
& \bm{W}_j = Z_j\vee \tilde{Z}_j \cdot \left\{\begin{aligned}
& +1, & 	Z_j > \tilde{Z}_j\vmadd{,}\\ & -1, & 	Z_j < \tilde{Z}_j\vmadd{.} 
\end{aligned} \right.
\end{align}
\vmcomment{Why is $W_j$ bolded above?}
Then a large $W_j$ implies ${\bm X}_j$ enters into the model early and does so before its knockoff, which is an evidence of non-null feature.

\subsection{Calculat\vmadd{ing} Data-dependent Threshold for Computed Statistics}
Let $q$ be the target FDR\vmdel{,}\vmadd{.} \vmdel{w}\vmadd{W}e \vmdel{can} define data-dependent threshold T as 
\begin{align}\label{eq:8}
T  = \min\left\{t\in\mathcal{W}: \frac{\#\{j, W_j\leq -t\}}{\#\{j, W_j\geq t\}\vee 1}\leq q \right\}
\end{align}
or $T=+\infty$ if this set is empty, where $\mathcal{W} = \{|W_j|;j=1,\dots,p \}$. 
\vmcomment{Explain the formula above in words.}
Then, we \vmdel{can} define the procedure of Knockoff variable selection as follows:

{\sc Definition 1. }(Knockoff). For any design matrix $\bm{X}\in\mathbb{R}^{n\times p}$ and response $\bm{y}\in \mathbb{R}^n$, 
\begin{enumerate}
	\item Create knockoff features $\bm{\tilde X}$ via (\ref{eq:4});
	\item Compute Statistics $W_j$ for all features via (\ref{eq:7});
	\item Compute threshold $T$ with the target FDR $q$ via (\ref{eq:8});
	\item Select features $\hat{S} =\{j: W_j\geq T\}$.
\end{enumerate}

With the Knockoff procedure, we have the main result for Knockoff in this paper.

{\sc Theorem 1. } {\it For any $q\in[0,1]$, the knockoff method satisfies }
\begin{align*}
& \mathbb{E}\left[\frac{\#\{j:\beta_j=0,\text{ and }j\in \hat{S} \}}{\#\{j:j\in \hat{S} \} + q^{-1}} \right]\leq q\vmadd{,}
\end{align*}
{\it where the expectation is taken over the Gaussian noise }\textbf{z} {\it in the model (\ref{eq:1}), while treating $\bm{X}$ and $\bm{\tilde{X}}$ as fixed. }

The ``modified FDR'' in Theorem 1 is very close to the FDR where $q^{-1}$ is small compared with the number of selected features. However, we still want to control the FDR exactly. For this purpose, \cite{barber2015controlling} suggest\vmdel{s} a slightly more conservative procedure.

{\sc Definition 2. }(Knockoff+). For any design matrix $\bm{X}\in\mathbb{R}^{n\times p}$ and response $\bm{y}\in \mathbb{R}^n$, 
follow steps 1 to 4 in {\sc Definition 1}, but in step 3, compute the threshold $T$ with the target FDR $q$ \vmdel{via \ref{eq:9}} \vmadd{as follows:}
\begin{align}\label{eq:9}
T  = \min\left\{t\in\mathcal{W}: \frac{1+\#\{j, W_j\leq -t\}}{\#\{j, W_j\geq t\}\vee 1}\leq q \right\}\vmadd{.}
\end{align}

With this slight change, the knockoff+ procedure controls the FDR exactly by the following theorem.
{\sc Theorem 1. } {\it For any $q\in[0,1]$, the knockoff+ method satisfies }
\begin{align*}
& \mathbb{E}\left[\frac{\#\{j:\beta_j=0,\text{ and }j\in \hat{S} \}}{\#\{j:j\in \hat{S} \}\vee 1} \right]\leq q \vmadd{.}
\end{align*}

\subsection{Extensions to $p<n<2p$}
When $n< 2p$, it's impossible to construct the knockoff variables $\tilde{\bm{X}}$ via equation (\ref{eq:4}), since there is no orthonormal matrix $\tilde{\bm{U}}$ to $\bm{X}$. 
However, as long as the noise level $\sigma$ is known or can be estimated accurately, knockoff filter can still be applied. 
The idea is to create ``observations'' to make up the number of observations to the level of $2p$. For doing this, assume for the last $2p-n$ observations, the feature variables are all $0$s. Then if we know the noise level $\sigma$ or we have an accurate estimate of it $\hat\sigma$, we can sample the response for the last $2p-n$ observations $\bm{y}'$ i.i.d. from $\mathcal{N}(0, \hat{\sigma}^2)$. Then approximately, 
\begin{align*}
& \begin{bmatrix}\bm{y} \\ \bm{y'}\end{bmatrix} \sim 
\mathcal{N}\left(\begin{bmatrix}\bm{X} \\ \bm{0}\end{bmatrix}\bm{\beta}, \sigma^2 \bm{I} \right)\vmadd{.}
\end{align*}
By construction, we have a linear model for $p$ variables and $2p$ observations. Thus, we can apply the knockoff filter in \textbf{Section }(2.3) to this row-augmented data. Also, we need to emphasize that the knockoff features $\tilde{\bm{X}}$ is constructed based on the augmented design matrix $\begin{bmatrix}\bm{X} \\ \bm{0}\end{bmatrix}$, which does not depend on the sampled responses $\bm{y'}$.
\section{Theory}
\vmcomment{I wouldn't separate Theory from Methods. If you want to prove something, do it as needed in the Methods section.}

\section{Results}

\section{Discussion}

\bibliography{stat572}

\end{document}









