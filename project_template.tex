\documentclass{uwstat572}

%%\setlength{\oddsidemargin}{0.25in}
%%\setlength{\textwidth}{6in}
%%\setlength{\topmargin}{0.5in}
%%\setlength{\textheight}{9in}

\usepackage{amsmath,amssymb}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{extarrows}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{setspace}

\renewcommand{\baselinestretch}{1.5} 
\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand\diag{\text{diag}}
\bibliographystyle{plainnat}

\begin{document}
%%\maketitle

\begin{center}
  {\LARGE Controlling False Discovery Rate via Knockoffs}\\\ \\
  {Peiran Liu \\ 
    Department of Statistics, University of Washington Seattle, WA, 98195, USA
  }
\end{center}



\begin{abstract}
  Put your project summary here.
\end{abstract}

\section{Introduction}
Hypothesis Testing is of great interest for statisticians, and testing simultaneously a family of hypotheses is essential in many areas, including variable selection, multiple comparisons etc. However, unguarded use of single testings procedure will result in a greatly amplified type I error. In practice, rarely we are interested only in whether all hypotheses are jointly true or not, which is testing combination of all null hypotheses. For most applications, we do inferences about single hypotheses, and wish to decide which ones can be rejected, which can be called discoveries. 

Traditionally, in such multiple hypotheses testing problems, we are interested in controlling the probability of erroneously rejecting the null hypotheses. In the most conservative prospect, scientists were interested in controlling the probability of erroneously rejecting even one of the true null hypotheses, which is defined as familywise error-rate (FWE). This criterion was widely used including \cite{hochberg2009multiple}, \cite{westfall1993resampling}. Controlling FWE requires each individual tests to be conducted at lower levels, such as Bonferroni procedures. 

However, with controlling FWE, the power of detecting true features will be greatly reduced as the number of hypotheses in the family increases, even when more powerful FWE controlling procedures are applied. In this point of view, instead of considering controlling FWE as the criterion, scientists are more interested in controlling the proportion of erroneous rejections among all rejections, which incurs the idea of False Discovery Rate.

\subsection{False Discovery Rate}
False Discovery Rate, suggested by \cite{benjamini1995controlling}, is a different point of view for how to look at those erroneous rejections. Instead of controlling the probability of erroneously rejecting even one hypothesis, Benjamini and Hochberg control the erroneous rejections by controlling the proportion of erroneous rejections among all rejections, which is the definition of False Discovery Rate. 

In the case where all hypotheses are true, controlling the FDR is equivalent to controlling the FWE, since when all hypotheses are true, any rejection will be erroneous rejection, which means that False Discovery Rate is 1 when there is any rejection and 0 when there is not. However, when the number of true null hypotheses is less than all hypotheses, controlling the FDR will not also controls FWE. 

To explain the definition of FDR clearer, assume we have $m$ different hypotheses to test, and of which $m_0$ are true. For any procedure in doing this multiple tests, if the result is in \textbf{Table }\ref{Tbl:1}:
\begin{table}[!htb]\label{Tbl:1}
	\centering
	\caption{Summary of Errors in Multiple Testing}
	\label{my-label}
	\begin{tabular}{l|ccc}
		& \specialcell{Declared\\ non-significant} & \specialcell{Declared\\ significant} & Total \\ \hline
		True Null Hypothesis	& U & V & $m_0$ \\
		Non-True null Hypothesis	& T & S & $m-m_0$ \\
		& $m-R$ & R & m
	\end{tabular}
\end{table}

Then, the definition of False Discover Rate is the proportion of rejected true nulls among all rejected hypothesis, which is a random variable $\bm{Q} = \bm{V}/(\bm{V}+\bm{S})$. Naturally, if no hypothesis is rejected, then we can define $\bm{Q} = 0$.

\subsection{False Discovery Rate Controlling in Variable Selection}
Similar to the ideas in multiple testing problems discussed in previous sections, \cite{barber2015controlling} defines the false discovery rate in variable selection problems, which is selecting features in regression models. Specifically, consider the linear regression model (\ref{eq:1})
\begin{align}\label{eq:1}
& \bm{y} = \bm{X}\bm{\beta}+\bm{z}
\end{align}

where as usual, variables $\bm{y}\in\mathbb{R}^n$ is a vector of responses, $\bm{X}\in \mathbb{R}^{n\times p}$ is a known design matrix, and $\bm{\beta}\in \mathbb{R}^p$ is the unknown vector of coefficients and $\bm{z}\sim \mathcal{N}(\bm{0}, \sigma^2\bm{I})$ is Gaussian noise. In this paper, authors restrict their attention to the case where $n\geq p$ as otherwise the model would not even be identifiable.

In practice, it's often the case that the size of true support of parameters $\bm{\beta}$ is small, that is, only a few features in $\bm{X}$ are expected to be associated with the outcome $y$ of interest. In terms of the linear model, this means that $|\{j; \beta_j \neq 0 \}|$ is small. The variable selection problem is that developing a procedure to choose $S\subset\{1,\dots, p\}$, such that more true features, whose $\beta_j$ is non-zero, are selected, and more null features are not selected in set $S$. In practice, we want to control the proportion of incorrectly selected features into our model. Similarly, we define the false discovery rate in variable selection as the proportion of erroneously selected features among all selected features.

If we consider a multiple testing problem with $p$ hypotheses $H_j: \bm{\beta}_j = 0$, where $j=1,2,\dots,p$, and rejecting hypotheses $H_j$ is statistically equivalent to selecting variable (feature) j. Then the false discovery rate defined in variable selection problem is equivalent to that in multiple testing problem. 

\subsection{The Knockoff Filter}
For controlling False Discovery Rate in variable selection problem, \cite{barber2015controlling} proposed a general FDR controlling procedure via knockoff variables. To summarize the ideas in this paper, they construct knockoff variables, which are constructed to imitate the correlation structure of the original features. On the other hand, the original features should be not so similar to its knockoff features, which will make its knockoff works better as a standard in the next variable selection procedure, such as Lasso \cite{tibshirani1996regression} applied in this paper.

Method proposed in \cite{barber2015controlling} is guaranteed to work under any fixed design $\bm{X} \in \mathbb{R}^{n\times p}$ as long as $n>p$ and $\bm{y}$ is linear Gaussian response. When these assumptions are satisfied, the procedure suggested in \cite{barber2015controlling} will theoretically control the false discovery rate with certain levels, and what's more, the power over-performs that in \cite{benjamini1995controlling}, and in \cite{benjamini2001control} in simulation studies.

\section{Methods}
\subsection{Construct Knockoffs}
For each feature $\bm{X}_j$ in the model, which is the $j$th column of $\bm{X}$, we construct a "knockoff" feature $\bm{\tilde X}_j$, such that 
\begin{align*}
& \tilde {\bm{X}}^T\tilde {\bm{X}} = \bm{X}^T\bm{X} = \bm{\Sigma} \\
& \tilde {\bm{X}}^T\bm{X} = \bm{\Sigma} - \text{diag}{(\bm{s})}
\end{align*}

where $\bm{s}$ is a $p$-dimensional non-negative vector. By this construction, we can have three properties of this knockoff features. The first is that the knockoff features $\bm{\tilde{X}}$ have the same correlation structure of the original features. Also, the correlation between distinct original and knockoff features is the same as the correlation between the corresponding distinct features because $\bm{X}_j^T\tilde{\bm{X}}_k = \bm{X}_j^T\bm{X}_k$ for all $j\neq k$. However, if we compare a feature $\bm{X}_j$ and its knockoff $\bm{\tilde X}_j$, the correlation is then:
\begin{align*}
& \bm{X}_j^T\tilde{\bm{X}}_j = \bm{\Sigma}_{jj} - s_j = 1-s_j
\end{align*}

when $\bm{X}$ is normalized. If we choose relatively large $s_j$, the knockoff features will be not so similar to its original features. Then, when we compare test statistic of one original true feature and its knockoff variable, the test statistic of the knockoff variable will be similar to the statistics of the null features, which are expected to be different from true features. Thus, with this construction, the knockoff features can be a good reference in the variable selection process. 

To choose the largest $s$ subject to the constraints $\diag\{\bm{s} \}\preceq 2\Sigma$ and $0\leq s_j\leq 1$, two different criterion can be applied. 
\begin{itemize}
\item {\it Equi-Correlated knockoffs:} If we assume the correlation between features and their knockoffs are equal, which means that all $s_j$ are equal, then we should choose $s_j$ in (\ref{eq:2.1})
\begin{align}\label{eq:2.1}
& s_j = 2\lambda_{\min}(\Sigma)\wedge 1
\end{align}
\item {\it SDP knockoffs:} Instead of assuming the correlation are equal, selecting $s$ so that the average correlation between features and their knockoffs is minimized is the second choice. This is done by solving the convex problem (\ref{eq:2.2}):
\begin{align}\label{eq:2.2}
&\begin{aligned}
& \text{Minimize }\sum_j(1-s_j)\\
&\text{Subject to }\quad 0\leq s_j\leq 1, \textbf{diag}(s) \preceq 2\Sigma
\end{aligned}
\end{align}
\end{itemize}

The construction of $\bm{\tilde X}$ can be done automatically by choose $\bm{s}\in\mathbb{R}_+^p$ satisfying $\diag\{\bm{s} \}\preceq 2\Sigma$, (which is a feasible convex optimization problem, \cite{boyd2004convex}, $\tilde{X}$ can be constructed as follows:
\begin{align}\label{eq:4}
& \tilde{\bm{X}} = \bm{X}(\bm{I}- \bm{\Sigma}^{-1}\text{diag}\{\bm{s} \}) + \bm{\tilde U}C 
\end{align}

where $\tilde{\bm{U}}$ is orthonormal $n\times p$ matrix to $\bm{X}$, and $\bm{C}$ is a Cholesky decomposition of $2\diag\{\bm{s}\} - \diag\{\bm{s}\} \Sigma^{-1} \diag\{\bm{s}\}$.

\subsection{Calculate Statistics for Each Pair of Variables via Lasso}
Lasso (\cite{tibshirani1996regression}) is a well-known variable selection methods in linear regressions, which is known to be asymptotically accurate for both variable selection and coefficient estimation. From the variable selection perspective, we consider the Lasso model (\ref{model:6})
\begin{align}\label{model:6}
& \hat{\bm{\beta}}(\lambda) = \arg\min_b \left\{\frac{1}{2}\|\bm{y} - \bm{X}\bm{b} \|_2^2 + \lambda\|\bm{b}\|_1 \right\}
\end{align}

Correspondingly, the test statistics for feature j is then:
\begin{align}
& Z_j = \sup\{\lambda; \hat{\bm{\beta}}_j(\lambda)=0 \}
\end{align}

which is likely to be small for null features but large for signals.
However, to quantify this and choose an appropriate threshold for variable selection, we need to use the knockoff variables to calibrate our threshold. Thus, if we replace $\bm{X}$ by $[\bm{X} \bm{\tilde{X}}]$ in (\ref{model:6}), we can get paired statistics $(Z_j, \tilde{Z}_j)$ as references. Thus, we can use the following statistics:
\begin{align}\label{eq:7}
& \bm{W}_j = Z_j\vee \tilde{Z}_j \cdot \left\{\begin{aligned}
& +1, & 	Z_j > \tilde{Z}_j\\ & -1, & 	Z_j < \tilde{Z}_j\\ 
\end{aligned} \right.
\end{align}

Then a large $W_j$ implies $X_j$ enters into the model early and does so before its knockoff, which is an evidence of non-null feature.

\subsection{Calculate Data-dependent Threshold for Computed Statistics}
Let $q$ be the target FDR, we can define data-dependent threshold T as 
\begin{align}\label{eq:8}
T  = \min\left\{t\in\mathcal{W}: \frac{\#\{j, W_j\leq -t\}}{\#\{j, W_j\geq t\}\vee 1}\leq q \right\}
\end{align}

or $T=+\infty$ if this set is empty, where $\mathcal{W} = \{|W_j|;j=1,\dots,p \}$. Then, we can define the procedure of Knockoff variable selection as follows:

{\sc Definition 1. }(Knockoff). For any design matrix $\bm{X}\in\mathbb{R}^{n\times p}$ and response $\bm{y}\in \mathbb{R}^n$, 
\begin{enumerate}
	\item Create knockoff features $\bm{\tilde X}$ via (\ref{eq:4});
	\item Compute Statistics $W_j$ for all features via (\ref{eq:7});
	\item Compute threshold $T$ with the target FDR $q$ via (\ref{eq:8});
	\item Select features $\hat{S} =\{j: W_j\geq T\}$.
\end{enumerate}

With the Knockoff procedure, we have the main result for Knockoff in this paper.

{\sc Theorem 1. } {\it For any $q\in[0,1]$, the knockoff method satisfies }
\begin{align*}
& \mathbb{E}\left[\frac{\#\{j:\beta_j=0,\text{ and }j\in \hat{S} \}}{\#\{j:j\in \hat{S} \} + q^{-1}} \right]\leq q
\end{align*}

{\it where the expectation is taken over the Gaussian noise }\textbf{z} {\it in the model (\ref{eq:1}), while treating $\bm{X}$ and $\bm{\tilde{X}}$ as fixed. }

The "modified FDR" in Theorem 1 is very close to the FDR where $q^{-1}$ is small compared with the number of selected features. However, we still want to control the FDR exactly. For this purpose, \cite{barber2015controlling} suggests a slightly more conservative procedure:

{\sc Definition 2. }(Knockoff+). For any design matrix $\bm{X}\in\mathbb{R}^{n\times p}$ and response $\bm{y}\in \mathbb{R}^n$, 
follow steps 1 to 4 in {\sc Definition 1}, but in step 3, compute the threshold $T$ with the target FDR $q$ via \ref{eq:9}
\begin{align}\label{eq:9}
T  = \min\left\{t\in\mathcal{W}: \frac{1+\#\{j, W_j\leq -t\}}{\#\{j, W_j\geq t\}\vee 1}\leq q \right\}
\end{align}

With this slight change, the knockoff+ procedure controls the FDR exactly by the following theorem.
{\sc Theorem 1. } {\it For any $q\in[0,1]$, the knockoff+ method satisfies }
\begin{align*}
& \mathbb{E}\left[\frac{\#\{j:\beta_j=0,\text{ and }j\in \hat{S} \}}{\#\{j:j\in \hat{S} \}\vee 1} \right]\leq q
\end{align*}

\subsection{Extensions to $p<n<2p$}
When $n< 2p$, it's impossible to construct the knockoff variables $\tilde{\bm{X}}$ via equation (\ref{eq:4}), since there is no orthonormal matrix $\tilde{\bm{U}}$ to $\bm{X}$. However, as long as the noise level $\sigma$ is known or can be estimated accurately, knockoff filter can still be applied. The idea is to create "observations" to make up the number of observations to the level of $2p$. For doing this, assume for the last $2p-n$ observations, the feature variables are all $0$s. Then if we know the noise level $\sigma$ or we have an accurate estimate of it $\hat\sigma$, we can sample the response for the last $2p-n$ observations $\bm{y}'$ i.i.d. from $\mathcal{N}(0, \hat{\sigma}^2)$. Then approximately, 
\begin{align*}
& \begin{bmatrix}\bm{y} \\ \bm{y'}\end{bmatrix} \sim 
\mathcal{N}\left(\begin{bmatrix}\bm{X} \\ \bm{0}\end{bmatrix}\bm{\beta}, \sigma^2 \bm{I} \right)
\end{align*}

By construction, we have a linear model for $p$ variables and $2p$ observations. Thus, we can apply the knockoff filter in \textbf{Section }(2.3) to this row-augmented data. Also, we need to emphasize that the knockoff features $\tilde{\bm{X}}$ is constructed based on the augmented design matrix $\begin{bmatrix}\bm{X} \\ \bm{0}\end{bmatrix}$, which does not depend on the sampled responses $\bm{y'}$.
\section{Theory}

\section{Results}

\section{Discussion}

\bibliography{stat572}

\end{document}









